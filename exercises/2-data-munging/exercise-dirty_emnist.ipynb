{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning, so we can build a classifiier\n",
    "\n",
    "### **Note:** Remember to check which environment this notebook is using!\n",
    "\n",
    "## Steps in this notebook\n",
    "1. Get the packages we need (install -> import)\n",
    "2. `emnist` pulls a clean dataset of handwritten characters\n",
    "    1. Clean dataframes are `raw_train` and `raw_test`\n",
    "    2. Each has only two columns\n",
    "        1. 'image' - 28x28 pixel array images of handwritten characters\n",
    "        2. 'label' - different number for class of digit: 0 -> 0, 10 -> A, 61 -> z\n",
    "3. Demonstrate basic plotting of the images\n",
    "4. Intentionally dirty this clean dataset\n",
    "    1. Dirty dataframes are `dirty_train` and `dirty_test`\n",
    "    2. Look through how the dirtying process works for ideas on how to clean up\n",
    "5. Space for whatever you're going to do next\n",
    "    1. Get cleaning with pandas\n",
    "    2. Build classifiers with the data you've cleaned\n",
    "    3. Skip data cleaning and build classifier with the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages\n",
    "\n",
    "Uncomment the followinig lines the first time running this notebook in this environment\n",
    "`%conda install pandas numpy emnist matplotlib`\n",
    "`%pip install emnist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import emnist\n",
    "from hashlib import sha1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data, and reshape it into a 28x28 array\n",
    "\n",
    "# The size of each image is 28x28 pixels\n",
    "size = 28 \n",
    "\n",
    "# Extract the training split as images and labels\n",
    "image, label = emnist.extract_training_samples('byclass')\n",
    "\n",
    "# Add columns for each pixel value (28x28 = 784 columns)\n",
    "raw_train = pd.DataFrame()\n",
    "\n",
    "# Add a column showing the label\n",
    "raw_train['label'] = label\n",
    "\n",
    "# Add a column with the image data as a 28x28 array\n",
    "raw_train['image'] = list(image)\n",
    "\n",
    "\n",
    "# Repeat for the test split\n",
    "image, label = emnist.extract_test_samples('byclass')\n",
    "raw_test = pd.DataFrame()\n",
    "raw_test['label'] = label\n",
    "raw_test['image'] = list(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot individual images using matplotlib\n",
    "plt.imshow(raw_train['image'][0], cmap='gray')\n",
    "plt.show() # Show the plot (optional with a single image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first row for each label\n",
    "firsts = raw_train.groupby('label').first().reset_index()\n",
    "\n",
    "# Build a plot with the first image for each label\n",
    "fig, ax = plt.subplots(7, 10, figsize=(10, 7))\n",
    "for i in range(62):\n",
    "    ax[i//10, i%10].imshow(firsts['image'][i], cmap='gray')\n",
    "    ax[i//10, i%10].axis('off')\n",
    "    ax[i//10, i%10].set_title(firsts['label'][i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting dirty\n",
    "\n",
    "Now that we've got the data in good shape, let's rough it up a little.\n",
    "\n",
    "- [x] Add `predicit` col with confidence probabilities from a previous model\n",
    "- [x] Numerical:\n",
    "    - [x] outlier\n",
    "    - [x] out-of-bounds\n",
    "- [x] Labels: missing(Null, None, \"\", \" \"), name that number, double-struck\n",
    "- [x] Image: zeroed, null? dimensions?\n",
    "- [x] Image: add noise\n",
    "- [x] Image: flip horizonally\n",
    "- [x] Duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's mess up the data a bit\n",
    "\n",
    "# Percent of the time something dirty happens (0.1%) for each method\n",
    "pct = 0.001 \n",
    "\n",
    "# Copy the splits into new dataframes to mess up\n",
    "dirty_train = raw_train.copy()\n",
    "dirty_test  = raw_test.copy()\n",
    "\n",
    "# Add a column for a hash of the images (should make it easier to compare them)\n",
    "dirty_train['image_hash'] = dirty_train['image'].apply(lambda x: sha1(x.tobytes()).hexdigest())\n",
    "dirty_test['image_hash']  =  dirty_test['image'].apply(lambda x: sha1(x.tobytes()).hexdigest())\n",
    "\n",
    "# For each row, 0.1% of the time, duplicate the row\n",
    "dirty_train = pd.concat([dirty_train, dirty_train.sample(frac=pct)])\n",
    "dirty_test  = pd.concat([dirty_test,   dirty_test.sample(frac=pct)])\n",
    "\n",
    "# For each row, 0.1% of the time, zero out the image array\n",
    "dirty_train['image'] = dirty_train['image'].apply(lambda x: np.zeros((size, size)) if np.random.rand() < pct else x)\n",
    "dirty_test['image']  =  dirty_test['image'].apply(lambda x: np.zeros((size, size)) if np.random.rand() < pct else x)\n",
    "\n",
    "# Add a column for classification scores from a previous model\n",
    "dirty_train['predict'] = np.random.normal(0.75, 0.1, dirty_train.shape[0])\n",
    "dirty_test['predict']  = np.random.normal(0.75, 0.1, dirty_test.shape[0])\n",
    "\n",
    "# For each row, 0.1% of the time, replace the predict column with a normal distribution centered on 0.25\n",
    "dirty_train['predict'] = dirty_train['predict'].apply(lambda x: np.random.normal(0.25, 0.1) if np.random.rand() < pct else x)\n",
    "dirty_test['predict']  =  dirty_test['predict'].apply(lambda x: np.random.normal(0.25, 0.1) if np.random.rand() < pct else x)\n",
    "\n",
    "# For each row, 0.1% of the time, add/subtract 1 to the predict column\n",
    "dirty_train['predict'] = dirty_train['predict'].apply(lambda x: x + 1 if np.random.rand() < pct/2 else x)\n",
    "dirty_test['predict']  =  dirty_test['predict'].apply(lambda x: x + 1 if np.random.rand() < pct/2 else x)\n",
    "dirty_train['predict'] = dirty_train['predict'].apply(lambda x: x - 1 if np.random.rand() < pct/2 else x)\n",
    "dirty_test['predict']  =  dirty_test['predict'].apply(lambda x: x - 1 if np.random.rand() < pct/2 else x)\n",
    "\n",
    "# For each row, 0.1% of the time, choose a column at random and set it to NaN\n",
    "dirty_train = dirty_train.apply(lambda row: row if np.random.rand() > pct else row.apply(lambda x: np.nan if np.random.rand() > 0.5 else x), axis=1)\n",
    "dirty_test  =  dirty_test.apply(lambda row: row if np.random.rand() > pct else row.apply(lambda x: np.nan if np.random.rand() > 0.5 else x), axis=1)\n",
    "\n",
    "# Mislabel 0.1% of the data with strings that look like labels or missing values (e.g., names like \"one\", numbers greater than 62, \" \")\n",
    "# Create a list of bad labels\n",
    "bad_labels = ['number', 'letter', 'maybe three?', 'think this is a seven', ' ', '', 'NaaN', 'Null'] + list(range(63, 100))\n",
    "# For 0.1% of the rows, randomly choose a bad label\n",
    "dirty_train['label'] = dirty_train['label'].apply(lambda x: np.random.choice(bad_labels) if np.random.rand() < pct else x)\n",
    "dirty_test['label']  =  dirty_test['label'].apply(lambda x: np.random.choice(bad_labels) if np.random.rand() < pct else x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TEMPLATE: For each row, PCT% of the time, randomly apply METHOD() to the COLUMN\n",
    "# df['COLUMN'] = dirty_train['column'].apply(lambda x: METHOD(x) if np.random.rand() < PCT else x)\n",
    "\n",
    "# Not applying these for now, but they're here if you want to try them\n",
    "\n",
    "# For each row, randomly decide whether to apply a random noise\n",
    "# dirty_train['image'] = dirty_train['image'].apply(lambda x: x + np.random.rand(size, size) if np.random.rand() < 0.1 else x)\n",
    "\n",
    "# For each row, randomly decide whether to flip the image horizontally\n",
    "#dirty_train['image'] = dirty_train['image'].apply(lambda x: np.flip(x, axis=1) if np.random.rand() < 0.1 else x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "The changes above were applied randomly, so we'll need to find them and make a plan to fix them.\n",
    "\n",
    "- [ ] Create a column to identify whether each row came from *train* or *test*\n",
    "- [ ] (optional) Merge the data into a single\n",
    "- [ ] Explore the data to understand what's in it\n",
    "- [ ] List potential data issues to fix\n",
    "    - Duplicate rows\n",
    "    - Missing values\n",
    "    - Outliers and out-of-bounds issues\n",
    "    - Label issues\n",
    "    - Image issues\n",
    "    - Zeroed values\n",
    "- [ ] Create a friendlier column for image labels\n",
    "- [ ] Recategorizing the labels into 'numbers' and 'letters'\n",
    "- [ ] Bin the prediction scores of the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start cleaning!\n",
    "\n",
    "# Labels! They're hard to understand as numbers, so let's map them to characters\n",
    "# We can do this by manually creating a dictionary:\n",
    "LABELS = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "          'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "          'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# Or generate the list of labels using the following code:\n",
    "# create the characters list, which is the digits, then uppercase, then lowercase\n",
    "chars = string.digits + string.ascii_uppercase + string.ascii_lowercase\n",
    "# create the dictionary mapping the numbers to the characters\n",
    "num_to_char = {i: chars[i] for i in range(len(chars))}\n",
    "\n",
    "# Add a column showing which split (train vs test) each row came from\n",
    "raw_train['split'] = 'train'\n",
    "raw_test['split']  = 'test'\n",
    "dirty_train['split'] = 'train'\n",
    "dirty_test['split'] = 'test'\n",
    "\n",
    "# Add a column for a hash of the images (might make it easier to compare them)\n",
    "dirty_train['image_hash'] = dirty_train['image'].apply(lambda x: sha1(np.ascontiguousarray(x)).hexdigest())\n",
    "dirty_test['image_hash']  =  dirty_test['image'].apply(lambda x: sha1(np.ascontiguousarray(x)).hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([dirty_test, dirty_train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.duplicated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e515f099f0da6a2737b6685058153dc4c80cb25a1e9fe8c5e831073d214043c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
