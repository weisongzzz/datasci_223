{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on `emnist`\n",
    "\n",
    "## 1. Create `Readme.md` to document your work\n",
    "\n",
    "Explain your choices, process, and outcomes.\n",
    "\n",
    "## 2. Classify all symbols\n",
    "\n",
    "### Choose a model\n",
    "\n",
    "Your choice of model! Choose wisely...\n",
    "\n",
    "### Train away!\n",
    "\n",
    "Is do you need to tune any parameters? Is the model expecting data in a different format?\n",
    "\n",
    "### Evaluate the model\n",
    "\n",
    "Evaluate the models on the test set, analyze the confusion matrix to see where the model performs well and where it struggles.\n",
    "\n",
    "### Investigate subsets\n",
    "\n",
    "On which classes does the model perform well? Poorly? Evaluate again, excluding easily confused symbols (such as 'O' and '0').\n",
    "\n",
    "### Improve performance\n",
    "\n",
    "Brainstorm for improving the performance. This could include trying different architectures, adding more layers, changing the loss function, or using data augmentation techniques.\n",
    "\n",
    "## 2. Classify digits vs. letters model showdown\n",
    "\n",
    "Perform a full showdown classifying digits vs letters:\n",
    "\n",
    "1. Create a column for whether each row is a digit or a letter\n",
    "2. Choose an evaluation metric \n",
    "3. Choose several candidate models to train\n",
    "4. Divide data to reserve a validation set that will NOT be used in training/testing\n",
    "5. K-fold train/test\n",
    "    1. Create train/test splits from the non-validation dataset \n",
    "    2. Train each candidate model (best practice: use the same split for all models)\n",
    "    3. Apply the model the the test split \n",
    "    4. (*Optional*) Perform hyper-parametric search\n",
    "    5. Record the model evaluation metrics\n",
    "    6. Repeat with a new train/test split\n",
    "6. Promote winner, apply model to validation set\n",
    "7. (*Optional*) Perform hyper-parametric search, if applicable\n",
    "8. Report model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emnist in c:\\users\\wilso\\onedrive\\documents from long ago\\datasci 223\\datasci_223\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\wilso\\onedrive\\documents from long ago\\datasci 223\\datasci_223\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (2.2.0)\n",
      "Collecting scikit-learn (from -r requirements.txt (line 3))\n",
      "  Downloading scikit_learn-1.4.0-1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\wilso\\onedrive\\documents from long ago\\datasci 223\\datasci_223\\.venv\\lib\\site-packages (from emnist->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\wilso\\onedrive\\documents from long ago\\datasci 223\\datasci_223\\.venv\\lib\\site-packages (from emnist->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wilso\\onedrive\\documents from long ago\\datasci 223\\datasci_223\\.venv\\lib\\site-packages (from emnist->-r requirements.txt (line 1)) (4.66.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\wilso\\onedrive\\documents from long ago\\datasci 223\\datasci_223\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wilso\\onedrive\\documents from long ago\\datasci 223\\datasci_223\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\wilso\\onedrive\\documents from long ago\\datasci 223\\datasci_223\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2023.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn->-r requirements.txt (line 3))\n",
      "  Downloading scipy-1.12.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.4 kB ? eta -:--:--\n",
      "     -------------------- ------------------- 30.7/60.4 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 60.4/60.4 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 3))\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->-r requirements.txt (line 3))\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wilso\\onedrive\\documents from long ago\\datasci 223\\datasci_223\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wilso\\onedrive\\documents from long ago\\datasci 223\\datasci_223\\.venv\\lib\\site-packages (from requests->emnist->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wilso\\onedrive\\documents from long ago\\datasci 223\\datasci_223\\.venv\\lib\\site-packages (from requests->emnist->-r requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wilso\\onedrive\\documents from long ago\\datasci 223\\datasci_223\\.venv\\lib\\site-packages (from requests->emnist->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wilso\\onedrive\\documents from long ago\\datasci 223\\datasci_223\\.venv\\lib\\site-packages (from requests->emnist->-r requirements.txt (line 1)) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\wilso\\onedrive\\documents from long ago\\datasci 223\\datasci_223\\.venv\\lib\\site-packages (from tqdm->emnist->-r requirements.txt (line 1)) (0.4.6)\n",
      "Downloading scikit_learn-1.4.0-1-cp312-cp312-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/10.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.7/10.6 MB 21.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.1/10.6 MB 25.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.3/10.6 MB 19.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.5/10.6 MB 20.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.1/10.6 MB 22.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.6/10.6 MB 24.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.7/10.6 MB 25.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 24.2 MB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 302.2/302.2 kB 18.2 MB/s eta 0:00:00\n",
      "Downloading scipy-1.12.0-cp312-cp312-win_amd64.whl (45.8 MB)\n",
      "   ---------------------------------------- 0.0/45.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.4/45.8 MB 30.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 2.8/45.8 MB 29.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 4.3/45.8 MB 30.1 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 5.6/45.8 MB 29.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 6.6/45.8 MB 29.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 7.9/45.8 MB 28.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 9.3/45.8 MB 28.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 10.2/45.8 MB 27.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 11.2/45.8 MB 27.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 12.5/45.8 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 13.8/45.8 MB 27.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 15.1/45.8 MB 26.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 16.4/45.8 MB 27.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 17.9/45.8 MB 27.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 19.1/45.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 20.7/45.8 MB 28.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 22.2/45.8 MB 29.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 23.7/45.8 MB 31.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 25.1/45.8 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 26.6/45.8 MB 32.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 28.0/45.8 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 29.4/45.8 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 31.0/45.8 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 32.4/45.8 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 33.3/45.8 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 34.4/45.8 MB 28.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 35.8/45.8 MB 28.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 37.2/45.8 MB 28.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 38.2/45.8 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 39.1/45.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 40.2/45.8 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 41.3/45.8 MB 24.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 42.3/45.8 MB 24.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 43.3/45.8 MB 24.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 44.1/45.8 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.0/45.8 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.7/45.8 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.8/45.8 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.8/45.8 MB 18.2 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.4.0 scipy-1.12.0 threadpoolctl-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# add packages\n",
    "%pip install -r requirements.txt\n",
    "import emnist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract both training and test from emnist\n",
    "image, label = emnist.extract_training_samples('byclass')\n",
    "raw_train = pd.DataFrame()\n",
    "raw_train['label'] = label\n",
    "raw_train['image'] = list(image)\n",
    "image, label = emnist.extract_test_samples('byclass')\n",
    "raw_test = pd.DataFrame()\n",
    "raw_test['label'] = label\n",
    "raw_test['image'] = list(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(697932, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(116323, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_train.shape)\n",
    "raw_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(814255, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(81426, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine training and test\n",
    "raw_data = pd.concat([raw_train,raw_test])\n",
    "print(raw_data.shape)\n",
    "#create a 10% subset of raw_data\n",
    "data = raw_data.sample(frac=0.1, replace=False, random_state=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def int_to_char(label):\n",
    "    \"\"\"Convert an integer label to the corresponding uppercase character.\"\"\"\n",
    "    if label < 10:\n",
    "        return str(label)\n",
    "    elif label < 36:\n",
    "        return chr(label - 10 + ord('A'))\n",
    "    else:\n",
    "        return chr(label - 36 + ord('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column \"class\" \n",
    "class_label = np.array([int_to_char(l) for l in data.iloc[:,0]])\n",
    "# make a copy of data added with class and image_flat cols\n",
    "data2 = data\n",
    "data2['class'] = class_label\n",
    "# add image_flat\n",
    "data2['image_flat'] = data2['image'].apply(lambda x: np.array(x).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train:test = 7:3\n",
    "train = data2.iloc[0:56998, :]\n",
    "test = data2.iloc[56998:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=20, random_state=5, max_depth=20)\n",
    "# Train and evaluate model\n",
    "rf_clf.fit(train['image_flat'].tolist(), train['class'])\n",
    "y_pred = rf_clf.predict(test['image_flat'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7480759783854594, 0.6389624182436264, 0.5219646237835093, 0.5433253811985825]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wilso\\OneDrive\\Documents from long ago\\Datasci 223\\datasci_223\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calculate performance metrics\n",
    "acc = accuracy_score(test['class'], y_pred)\n",
    "prec = precision_score(test['class'], y_pred, average='macro')\n",
    "rec = recall_score(test['class'], y_pred, average='macro')\n",
    "f1 = f1_score(test['class'], y_pred, average='macro')\n",
    "cm = confusion_matrix(test['class'], y_pred)\n",
    "print([acc,prec,rec,f1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
